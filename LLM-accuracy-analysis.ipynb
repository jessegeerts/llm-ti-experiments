{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Any\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMResultsAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the analyzer with dataset paths\"\"\"\n",
    "        self.datasets = {\n",
    "            'congruent': 'congruent_incongruent_1000/test/comparison_congruent_size_test.tfrecord',\n",
    "            'incongruent': 'congruent_incongruent_1000/test/comparison_incongruent_size_test.tfrecord',\n",
    "            'random': 'congruent_incongruent_1000/test/comparison_random_string_size_test.tfrecord',\n",
    "            'permuted': 'congruent_incongruent_1000/test/comparison_permuted_size_test.tfrecord'\n",
    "        }\n",
    "        \n",
    "        self.prompt_types = ['baseline', 'numberline', 'road', 'circle', 'clusters', \n",
    "                            '3d_space', 'cloud', 'hyperbolic', 'cot']\n",
    "        \n",
    "        self.ground_truth = self.load_ground_truth()\n",
    "    \n",
    "    def parse_tfrecord_example(self, serialized_item: bytes) -> Dict[str, Any]:\n",
    "        \"\"\"Parse a single TFRecord example\"\"\"\n",
    "        feature_description = {\n",
    "            'question': tf.io.FixedLenFeature([], tf.string),\n",
    "            'answer': tf.io.FixedLenFeature([], tf.string),\n",
    "            'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "            'question_only': tf.io.FixedLenFeature([], tf.string),\n",
    "            'alternative_answers': tf.io.VarLenFeature(tf.string),\n",
    "            'index': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        return tf.io.parse_single_example(serialized_item, feature_description)\n",
    "    \n",
    "    def load_ground_truth(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load ground truth answers from tfrecord files\"\"\"\n",
    "        ground_truth = {}\n",
    "        \n",
    "        for condition, filepath in self.datasets.items():\n",
    "            print(f\"Loading ground truth for {condition}...\")\n",
    "            dataset = tf.data.TFRecordDataset(filepath)\n",
    "            # For permuted dataset, use simpler parsing\n",
    "            if condition == 'permuted':\n",
    "                feature_description = {\n",
    "                    'question': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'answer': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "                }\n",
    "            else:\n",
    "                # For original datasets, include all fields\n",
    "                feature_description = {\n",
    "                    'question': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'answer': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'question_only': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'alternative_answers': tf.io.VarLenFeature(tf.string),\n",
    "                    'index': tf.io.FixedLenFeature([], tf.int64),\n",
    "                }\n",
    "            \n",
    "            parsed_dataset = dataset.map(lambda x: tf.io.parse_single_example(x, feature_description))\n",
    "            \n",
    "            answers = []\n",
    "            for record in parsed_dataset.take(1000):  # Take 1000 examples\n",
    "                answer = record['answer'].numpy().decode('utf-8').lower()\n",
    "                answers.append(answer)\n",
    "            \n",
    "            ground_truth[condition] = answers\n",
    "            print(f\"  Loaded {len(answers)} answers\")\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def extract_answer_from_response(self, response: str) -> str:\n",
    "        \"\"\"Extract yes/no answer from model response\"\"\"\n",
    "        response_lower = response.lower().strip()\n",
    "        \n",
    "        # Look for explicit answer patterns\n",
    "        patterns = [\n",
    "            r'answer:\\s*(yes|no)',\n",
    "            r'answer\\s*:\\s*(yes|no)',\n",
    "            r'\\*\\*answer:\\s*(yes|no)\\*\\*',\n",
    "            r'\\*\\*answer\\s*:\\s*(yes|no)\\*\\*',\n",
    "            r'therefore.*answer is\\s*(yes|no)',\n",
    "            r'the answer is\\s*\\*?\\*?(yes|no)',\n",
    "            r'answer yes or no\\.\\s*(yes|no)',\n",
    "            r'^(yes|no)$',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response_lower, re.MULTILINE | re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).lower()\n",
    "        \n",
    "        # Fallback: look for last occurrence of yes or no\n",
    "        if 'yes' in response_lower or 'no' in response_lower:\n",
    "            yes_pos = response_lower.rfind('yes')\n",
    "            no_pos = response_lower.rfind('no')\n",
    "            if yes_pos > no_pos:\n",
    "                return 'yes'\n",
    "            elif no_pos > yes_pos:\n",
    "                return 'no'\n",
    "        \n",
    "        return 'unclear'  # Could not extract answer\n",
    "    \n",
    "    def parse_model_results(self, results_file: str, model_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Parse results file from a model\"\"\"\n",
    "        print(f\"\\nParsing results for {model_name}...\")\n",
    "        \n",
    "        # Read the results file\n",
    "        with open(results_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Split by delimiter\n",
    "        responses = content.split('---DELIM---')\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # We expect 36,000 responses (4 conditions × 9 prompts × 1000 examples)\n",
    "        expected_per_condition = 9 * 1000\n",
    "        \n",
    "        for condition_idx, condition in enumerate(['congruent', 'incongruent', 'random', 'permuted']):\n",
    "            for prompt_idx, prompt_type in enumerate(self.prompt_types):\n",
    "                for example_idx in range(1000):\n",
    "                    # Calculate overall index\n",
    "                    overall_idx = (condition_idx * expected_per_condition + \n",
    "                                 prompt_idx * 1000 + example_idx)\n",
    "                    \n",
    "                    if overall_idx >= len(responses):\n",
    "                        print(f\"Warning: Missing response at index {overall_idx}\")\n",
    "                        continue\n",
    "                    \n",
    "                    response = responses[overall_idx].strip()\n",
    "                    \n",
    "                    # Extract predicted answer\n",
    "                    predicted = self.extract_answer_from_response(response)\n",
    "                    \n",
    "                    # Get ground truth\n",
    "                    ground_truth = self.ground_truth[condition][example_idx]\n",
    "                    \n",
    "                    # Check if correct\n",
    "                    is_correct = 1 if predicted == ground_truth else 0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'model': model_name,\n",
    "                        'condition': condition,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'example_idx': example_idx,\n",
    "                        'predicted': predicted,\n",
    "                        'ground_truth': ground_truth,\n",
    "                        'is_correct': is_correct,\n",
    "                        'response_length': len(response)\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"  Parsed {len(df)} responses\")\n",
    "        return df\n",
    "    \n",
    "    def calculate_confidence_interval(self, accuracies: List[float], confidence: float = 0.95) -> tuple:\n",
    "        \"\"\"Calculate confidence interval for accuracy\"\"\"\n",
    "        n = len(accuracies)\n",
    "        if n == 0:\n",
    "            return 0, 0\n",
    "        \n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_err = np.std(accuracies, ddof=1) / np.sqrt(n)\n",
    "        \n",
    "        # Use t-distribution for small samples\n",
    "        t_val = scipy_stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "        margin_error = t_val * std_err\n",
    "        \n",
    "        return mean_acc - margin_error, mean_acc + margin_error\n",
    "    \n",
    "    def analyze_single_model(self, df: pd.DataFrame, model_name: str):\n",
    "        \"\"\"Analyze results for a single model\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ANALYSIS FOR {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Calculate accuracy by condition and prompt type\n",
    "        accuracy_summary = []\n",
    "        \n",
    "        for condition in df['condition'].unique():\n",
    "            for prompt_type in df['prompt_type'].unique():\n",
    "                subset = df[(df['condition'] == condition) & \n",
    "                           (df['prompt_type'] == prompt_type)]\n",
    "                \n",
    "                if len(subset) > 0:\n",
    "                    acc = subset['is_correct'].mean()\n",
    "                    ci_low, ci_high = self.calculate_confidence_interval(subset['is_correct'].values)\n",
    "                    \n",
    "                    accuracy_summary.append({\n",
    "                        'condition': condition,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'accuracy': acc,\n",
    "                        'ci_low': ci_low,\n",
    "                        'ci_high': ci_high,\n",
    "                        'n': len(subset)\n",
    "                    })\n",
    "        \n",
    "        accuracy_df = pd.DataFrame(accuracy_summary)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nAccuracy by Condition and Prompt Type:\")\n",
    "        print(\"-\" * 60)\n",
    "        for condition in ['congruent', 'incongruent', 'random', 'permuted']:\n",
    "            print(f\"\\n{condition.upper()}:\")\n",
    "            condition_data = accuracy_df[accuracy_df['condition'] == condition]\n",
    "            for _, row in condition_data.iterrows():\n",
    "                print(f\"  {row['prompt_type']:12}: {row['accuracy']:.3f} \"\n",
    "                      f\"(CI: [{row['ci_low']:.3f}, {row['ci_high']:.3f}])\")\n",
    "        \n",
    "        return accuracy_df\n",
    "    \n",
    "    def compare_geometries(self, df: pd.DataFrame, model_name: str):\n",
    "        \"\"\"Compare linear vs non-linear geometries\"\"\"\n",
    "        linear_geometries = ['numberline', 'road']\n",
    "        nonlinear_geometries = ['circle', 'clusters', '3d_space', 'cloud', 'hyperbolic']\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for condition in df['condition'].unique():\n",
    "            # Baseline\n",
    "            baseline = df[(df['condition'] == condition) & \n",
    "                         (df['prompt_type'] == 'baseline')]\n",
    "            baseline_acc = baseline['is_correct'].mean() if len(baseline) > 0 else 0\n",
    "            \n",
    "            # Linear geometries\n",
    "            linear = df[(df['condition'] == condition) & \n",
    "                       (df['prompt_type'].isin(linear_geometries))]\n",
    "            linear_acc = linear['is_correct'].mean() if len(linear) > 0 else 0\n",
    "            \n",
    "            # Non-linear geometries\n",
    "            nonlinear = df[(df['condition'] == condition) & \n",
    "                          (df['prompt_type'].isin(nonlinear_geometries))]\n",
    "            nonlinear_acc = nonlinear['is_correct'].mean() if len(nonlinear) > 0 else 0\n",
    "            \n",
    "            # CoT\n",
    "            cot = df[(df['condition'] == condition) & \n",
    "                    (df['prompt_type'] == 'cot')]\n",
    "            cot_acc = cot['is_correct'].mean() if len(cot) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'condition': condition,\n",
    "                'baseline': baseline_acc,\n",
    "                'linear': linear_acc,\n",
    "                'nonlinear': nonlinear_acc,\n",
    "                'cot': cot_acc,\n",
    "                'linear_improvement': linear_acc - baseline_acc,\n",
    "                'nonlinear_improvement': nonlinear_acc - baseline_acc,\n",
    "                'cot_improvement': cot_acc - baseline_acc\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\n{model_name} - Geometry Comparison:\")\n",
    "        print(\"-\" * 60)\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            print(f\"{row['condition']:12}: Baseline {row['baseline']:.3f} | \"\n",
    "                  f\"Linear {row['linear']:.3f} (Δ={row['linear_improvement']:+.3f}) | \"\n",
    "                  f\"Non-linear {row['nonlinear']:.3f} (Δ={row['nonlinear_improvement']:+.3f}) | \"\n",
    "                  f\"CoT {row['cot']:.3f} (Δ={row['cot_improvement']:+.3f})\")\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    def create_model_comparison_plot(self, all_results: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create visualization comparing all models\"\"\"\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        conditions = ['congruent', 'incongruent', 'random', 'permuted']\n",
    "        \n",
    "        for idx, (model_name, df) in enumerate(all_results.items()):\n",
    "            if idx >= 9:\n",
    "                break\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Calculate accuracy by condition and prompt category\n",
    "            linear = ['numberline', 'road']\n",
    "            nonlinear = ['circle', 'clusters', '3d_space', 'cloud', 'hyperbolic']\n",
    "            \n",
    "            data_for_plot = []\n",
    "            for condition in conditions:\n",
    "                baseline_acc = df[(df['condition'] == condition) & \n",
    "                                 (df['prompt_type'] == 'baseline')]['accuracy'].mean()\n",
    "                linear_acc = df[(df['condition'] == condition) & \n",
    "                              (df['prompt_type'].isin(linear))]['accuracy'].mean()\n",
    "                nonlinear_acc = df[(df['condition'] == condition) & \n",
    "                                 (df['prompt_type'].isin(nonlinear))]['accuracy'].mean()\n",
    "                cot_acc = df[(df['condition'] == condition) & \n",
    "                            (df['prompt_type'] == 'cot')]['accuracy'].mean()\n",
    "                \n",
    "                data_for_plot.append({\n",
    "                    'condition': condition,\n",
    "                    'Baseline': baseline_acc,\n",
    "                    'Linear': linear_acc,\n",
    "                    'Non-linear': nonlinear_acc,\n",
    "                    'CoT': cot_acc\n",
    "                })\n",
    "            \n",
    "            plot_df = pd.DataFrame(data_for_plot)\n",
    "            plot_df.set_index('condition')[['Baseline', 'Linear', 'Non-linear', 'CoT']].plot(\n",
    "                kind='bar', ax=ax, width=0.8\n",
    "            )\n",
    "            \n",
    "            ax.set_title(model_name.replace('_', ' ').title())\n",
    "            ax.set_xlabel('Condition')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Model Comparison: Geometry Effects on Transitive Inference', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def run_analysis(self, model_files: Dict[str, str]):\n",
    "        \"\"\"Run complete analysis for all models\"\"\"\n",
    "        all_results = {}\n",
    "        all_comparisons = {}\n",
    "        \n",
    "        for model_name, results_file in model_files.items():\n",
    "            if not Path(results_file).exists():\n",
    "                print(f\"Skipping {model_name} - file not found: {results_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse results\n",
    "            df = self.parse_model_results(results_file, model_name)\n",
    "            \n",
    "            # Analyze\n",
    "            accuracy_df = self.analyze_single_model(df, model_name)\n",
    "            comparison_df = self.compare_geometries(df, model_name)\n",
    "            \n",
    "            # Store results\n",
    "            all_results[model_name] = accuracy_df\n",
    "            all_comparisons[model_name] = comparison_df\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(f'{model_name}_detailed_results.csv', index=False)\n",
    "            accuracy_df.to_csv(f'{model_name}_accuracy_summary.csv', index=False)\n",
    "            comparison_df.to_csv(f'{model_name}_geometry_comparison.csv', index=False)\n",
    "        \n",
    "        # Create comparison plot\n",
    "        self.create_model_comparison_plot(all_results)\n",
    "        \n",
    "        return all_results, all_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main analysis function\"\"\"\n",
    "    analyzer = LLMResultsAnalyzer()\n",
    "    \n",
    "    # Define model result files\n",
    "    model_files = {\n",
    "        'gemini_2.5_flash': 'llm_output/gemini-2.5-flash-all_prompts.txt',\n",
    "        'gemini_2.5_flash_lite': 'llm_output/gemini-2.5-flash-lite-all_prompts.txt',\n",
    "        'gemini_2.5_pro': 'llm_output/gemini-2.5-pro-all_prompts.txt',\n",
    "        'gemma3_1b': 'llm_output/gemma3_1b-all_prompts.txt',\n",
    "        'gemma3_4b': 'llm_output/gemma3_4b-all_prompts.txt',\n",
    "        'gemma3_12b': 'llm_output/gemma3_12b-all_prompts.txt',\n",
    "        'gemma3_27b': 'llm_output/gemma3_27b-all_prompts.txt'\n",
    "    }\n",
    "    \n",
    "    # Run analysis\n",
    "    all_results, all_comparisons = analyzer.run_analysis(model_files)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nFiles generated:\")\n",
    "    print(\"- [model_name]_detailed_results.csv - Full results for each model\")\n",
    "    print(\"- [model_name]_accuracy_summary.csv - Accuracy by condition and prompt\")\n",
    "    print(\"- [model_name]_geometry_comparison.csv - Linear vs non-linear comparison\")\n",
    "    print(\"- all_models_comparison.png - Visual comparison across all models\")\n",
    "    \n",
    "    return all_results, all_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, comparisons = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Define model result files\n",
    "model_files = {\n",
    "    'gemini_2.5_flash': 'llm_output/gemini-2.5-flash-all_prompts.txt',\n",
    "    'gemini_2.5_flash_lite': 'llm_output/gemini-2.5-flash-lite-all_prompts.txt',\n",
    "    'gemini_2.5_pro': 'llm_output/gemini-2.5-pro-all_prompts.txt',\n",
    "    'gemma3_1b': 'llm_output/gemma3_1b-all_prompts.txt',\n",
    "    'gemma3_4b': 'llm_output/gemma3_4b-all_prompts.txt',\n",
    "    'gemma3_12b': 'llm_output/gemma3_12b-all_prompts.txt',\n",
    "    'gemma3_27b': 'llm_output/gemma3_27b-all_prompts.txt'\n",
    "}\n",
    "\n",
    "analyzer = LLMResultsAnalyzer()\n",
    "# Run analysis\n",
    "all_results, all_comparisons = analyzer.run_analysis(model_files)\n",
    "\n",
    "analyzer.create_model_comparison_plot(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    'gemini_2.5_flash': 'llm_output/gemini-2.5-flash-all_prompts.txt',\n",
    "    'gemini_2.5_flash_lite': 'llm_output/gemini-2.5-flash-lite-all_prompts.txt',\n",
    "    'gemini_2.5_pro': 'llm_output/gemini-2.5-pro-all_prompts.txt',\n",
    "    'gemma3_1b': 'llm_output/gemma3_1b-all_prompts.txt',\n",
    "    'gemma3_4b': 'llm_output/gemma3_4b-all_prompts.txt',\n",
    "    'gemma3_12b': 'llm_output/gemma3_12b-all_prompts.txt',\n",
    "    'gemma3_27b': 'llm_output/gemma3_27b-all_prompts.txt'\n",
    "}\n",
    "\n",
    "model_names = list(model_files.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    df = pd.read_csv(model_names[i] + '_accuracy_summary.csv')\n",
    "    \n",
    "    print(model_names[i])\n",
    "    \n",
    "    # Preprocess\n",
    "    df['yerr_lower'] = df['accuracy'] - df['ci_low']\n",
    "    df['yerr_upper'] = df['ci_high'] - df['accuracy']\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    g = sns.barplot(ax=ax, data=df, x='prompt_type', y='accuracy', hue='condition')\n",
    "    # g._legend.set_bbox_to_anchor((1.1, 0.5))  # x, y — move to right center\n",
    "    \n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.title(model_name)\n",
    "    # Add error bars manually\n",
    "    # Iterate over bars and add errorbars\n",
    "    for bar, (_, row) in zip(ax.patches, df.iterrows()):\n",
    "        # Calculate the center of each bar\n",
    "        x = bar.get_x() + bar.get_width() / 2\n",
    "        y = bar.get_height()\n",
    "        \n",
    "        # Add error bar using asymmetric values\n",
    "        ax.errorbar(\n",
    "            x, y,\n",
    "            yerr=[[row['yerr_lower']], [row['yerr_upper']]],\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=5,\n",
    "            linewidth=1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    fig.legend(bbox_to_anchor=(1.15,0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'accuracy_{model_name}.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model_name in enumerate(model_names):\n",
    "    \n",
    "    df = pd.read_csv(model_name + '_geometry_comparison.csv')\n",
    "    \n",
    "    df\n",
    "    df_pivoted = df[['condition', 'baseline', 'linear', 'nonlinear', 'cot']].melt(id_vars='condition',\n",
    "                                                                                 var_name='prompt_type',\n",
    "                                                                                 value_name='accuracy')\n",
    "    df_pivoted\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ylim((0,1))\n",
    "    sns.barplot(ax=ax, data=df_pivoted, x='prompt_type', y='accuracy', hue='condition', palette='husl')\n",
    "    plt.title(model_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'geometry_comparison_{model_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['condition', 'baseline', 'linear', 'nonlinear', 'cot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "n_unclear_responses = {}\n",
    "for model_name in model_names:\n",
    "    df = pd.read_csv(model_name + '_detailed_results.csv').reset_index()\n",
    "\n",
    "    n_unclear_responses[model_name] = np.sum(df['predicted']=='unclear')\n",
    "    dfs.append(df)\n",
    "n_unclear_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detailed_results_df = pd.concat(dfs)\n",
    "all_detailed_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=all_detailed_results_df, y='response_length', x='model', ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=all_detailed_results_df, y='response_length', x='model', hue='condition', ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=all_detailed_results_df, y='response_length', x='model', hue='prompt_type', ax=ax, palette='husl')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "next, look at distribution of response lengths, per model. are there two types of responses (quick answer versus reasoning trace), or is it more a continuum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "mn = model_names[0]\n",
    "sns.histplot(data=all_detailed_results_df[all_detailed_results_df.model==mn], x='response_length')\n",
    "plt.title(mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model_name in enumerate(model_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.histplot(ax=ax, data=all_detailed_results_df[all_detailed_results_df.model==model_name], x='response_length')\n",
    "    plt.title(model_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'response_length_{model_name}.png')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
