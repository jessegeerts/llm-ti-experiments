{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Any\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMResultsAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the analyzer with dataset paths\"\"\"\n",
    "        self.datasets = {\n",
    "            'congruent': 'congruent_incongruent_1000/test/comparison_congruent_size_test.tfrecord',\n",
    "            'incongruent': 'congruent_incongruent_1000/test/comparison_incongruent_size_test.tfrecord',\n",
    "            'random': 'congruent_incongruent_1000/test/comparison_random_string_size_test.tfrecord',\n",
    "            'permuted': 'congruent_incongruent_1000/test/comparison_permuted_size_test.tfrecord'\n",
    "        }\n",
    "        \n",
    "        self.prompt_types = ['baseline', 'numberline', 'road', 'circle', 'clusters', \n",
    "                            '3d_space', 'cloud', 'hyperbolic', 'cot']\n",
    "        \n",
    "        self.ground_truth = self.load_ground_truth()\n",
    "    \n",
    "    def parse_tfrecord_example(self, serialized_item: bytes) -> Dict[str, Any]:\n",
    "        \"\"\"Parse a single TFRecord example\"\"\"\n",
    "        feature_description = {\n",
    "            'question': tf.io.FixedLenFeature([], tf.string),\n",
    "            'answer': tf.io.FixedLenFeature([], tf.string),\n",
    "            'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "            'question_only': tf.io.FixedLenFeature([], tf.string),\n",
    "            'alternative_answers': tf.io.VarLenFeature(tf.string),\n",
    "            'index': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        return tf.io.parse_single_example(serialized_item, feature_description)\n",
    "    \n",
    "    def load_ground_truth(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load ground truth answers from tfrecord files\"\"\"\n",
    "        ground_truth = {}\n",
    "        \n",
    "        for condition, filepath in self.datasets.items():\n",
    "            print(f\"Loading ground truth for {condition}...\")\n",
    "            dataset = tf.data.TFRecordDataset(filepath)\n",
    "            # For permuted dataset, use simpler parsing\n",
    "            if condition == 'permuted':\n",
    "                feature_description = {\n",
    "                    'question': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'answer': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "                }\n",
    "            else:\n",
    "                # For original datasets, include all fields\n",
    "                feature_description = {\n",
    "                    'question': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'answer': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'metadata': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'question_only': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'alternative_answers': tf.io.VarLenFeature(tf.string),\n",
    "                    'index': tf.io.FixedLenFeature([], tf.int64),\n",
    "                }\n",
    "            \n",
    "            parsed_dataset = dataset.map(lambda x: tf.io.parse_single_example(x, feature_description))\n",
    "            \n",
    "            answers = []\n",
    "            for record in parsed_dataset.take(1000):  # Take 1000 examples\n",
    "                answer = record['answer'].numpy().decode('utf-8').lower()\n",
    "                answers.append(answer)\n",
    "            \n",
    "            ground_truth[condition] = answers\n",
    "            print(f\"  Loaded {len(answers)} answers\")\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def extract_answer_from_response(self, response: str) -> str:\n",
    "        \"\"\"Extract yes/no answer from model response\"\"\"\n",
    "        response_lower = response.lower().strip()\n",
    "        \n",
    "        # Look for explicit answer patterns\n",
    "        patterns = [\n",
    "            r'answer:\\s*(yes|no)',\n",
    "            r'answer\\s*:\\s*(yes|no)',\n",
    "            r'\\*\\*answer:\\s*(yes|no)\\*\\*',\n",
    "            r'\\*\\*answer\\s*:\\s*(yes|no)\\*\\*',\n",
    "            r'therefore.*answer is\\s*(yes|no)',\n",
    "            r'the answer is\\s*\\*?\\*?(yes|no)',\n",
    "            r'answer yes or no\\.\\s*(yes|no)',\n",
    "            r'^(yes|no)$',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response_lower, re.MULTILINE | re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).lower()\n",
    "        \n",
    "        # Fallback: look for last occurrence of yes or no\n",
    "        if 'yes' in response_lower or 'no' in response_lower:\n",
    "            yes_pos = response_lower.rfind('yes')\n",
    "            no_pos = response_lower.rfind('no')\n",
    "            if yes_pos > no_pos:\n",
    "                return 'yes'\n",
    "            elif no_pos > yes_pos:\n",
    "                return 'no'\n",
    "        \n",
    "        return 'unclear'  # Could not extract answer\n",
    "    \n",
    "    def parse_model_results(self, results_file: str, model_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Parse results file from a model\"\"\"\n",
    "        print(f\"\\nParsing results for {model_name}...\")\n",
    "        \n",
    "        # Read the results file\n",
    "        with open(results_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Split by delimiter\n",
    "        responses = content.split('---DELIM---')\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # We expect 36,000 responses (4 conditions × 9 prompts × 1000 examples)\n",
    "        expected_per_condition = 9 * 1000\n",
    "        \n",
    "        for condition_idx, condition in enumerate(['congruent', 'incongruent', 'random', 'permuted']):\n",
    "            for prompt_idx, prompt_type in enumerate(self.prompt_types):\n",
    "                for example_idx in range(1000):\n",
    "                    # Calculate overall index\n",
    "                    overall_idx = (condition_idx * expected_per_condition + \n",
    "                                 prompt_idx * 1000 + example_idx)\n",
    "                    \n",
    "                    if overall_idx >= len(responses):\n",
    "                        print(f\"Warning: Missing response at index {overall_idx}\")\n",
    "                        continue\n",
    "                    \n",
    "                    response = responses[overall_idx].strip()\n",
    "                    \n",
    "                    # Extract predicted answer\n",
    "                    predicted = self.extract_answer_from_response(response)\n",
    "                    \n",
    "                    # Get ground truth\n",
    "                    ground_truth = self.ground_truth[condition][example_idx]\n",
    "                    \n",
    "                    # Check if correct\n",
    "                    is_correct = 1 if predicted == ground_truth else 0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'model': model_name,\n",
    "                        'condition': condition,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'example_idx': example_idx,\n",
    "                        'predicted': predicted,\n",
    "                        'ground_truth': ground_truth,\n",
    "                        'is_correct': is_correct,\n",
    "                        'response_length': len(response)\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"  Parsed {len(df)} responses\")\n",
    "        return df\n",
    "    \n",
    "    def calculate_confidence_interval(self, accuracies: List[float], confidence: float = 0.95) -> tuple:\n",
    "        \"\"\"Calculate confidence interval for accuracy\"\"\"\n",
    "        n = len(accuracies)\n",
    "        if n == 0:\n",
    "            return 0, 0\n",
    "        \n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_err = np.std(accuracies, ddof=1) / np.sqrt(n)\n",
    "        \n",
    "        # Use t-distribution for small samples\n",
    "        t_val = scipy_stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "        margin_error = t_val * std_err\n",
    "        \n",
    "        return mean_acc - margin_error, mean_acc + margin_error\n",
    "    \n",
    "    def analyze_single_model(self, df: pd.DataFrame, model_name: str):\n",
    "        \"\"\"Analyze results for a single model\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ANALYSIS FOR {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Calculate accuracy by condition and prompt type\n",
    "        accuracy_summary = []\n",
    "        \n",
    "        for condition in df['condition'].unique():\n",
    "            for prompt_type in df['prompt_type'].unique():\n",
    "                subset = df[(df['condition'] == condition) & \n",
    "                           (df['prompt_type'] == prompt_type)]\n",
    "                \n",
    "                if len(subset) > 0:\n",
    "                    acc = subset['is_correct'].mean()\n",
    "                    ci_low, ci_high = self.calculate_confidence_interval(subset['is_correct'].values)\n",
    "                    \n",
    "                    accuracy_summary.append({\n",
    "                        'condition': condition,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'accuracy': acc,\n",
    "                        'ci_low': ci_low,\n",
    "                        'ci_high': ci_high,\n",
    "                        'n': len(subset)\n",
    "                    })\n",
    "        \n",
    "        accuracy_df = pd.DataFrame(accuracy_summary)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nAccuracy by Condition and Prompt Type:\")\n",
    "        print(\"-\" * 60)\n",
    "        for condition in ['congruent', 'incongruent', 'random', 'permuted']:\n",
    "            print(f\"\\n{condition.upper()}:\")\n",
    "            condition_data = accuracy_df[accuracy_df['condition'] == condition]\n",
    "            for _, row in condition_data.iterrows():\n",
    "                print(f\"  {row['prompt_type']:12}: {row['accuracy']:.3f} \"\n",
    "                      f\"(CI: [{row['ci_low']:.3f}, {row['ci_high']:.3f}])\")\n",
    "        \n",
    "        return accuracy_df\n",
    "    \n",
    "    def compare_geometries(self, df: pd.DataFrame, model_name: str):\n",
    "        \"\"\"Compare linear vs non-linear geometries\"\"\"\n",
    "        linear_geometries = ['numberline', 'road']\n",
    "        nonlinear_geometries = ['circle', 'clusters', '3d_space', 'cloud', 'hyperbolic']\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for condition in df['condition'].unique():\n",
    "            # Baseline\n",
    "            baseline = df[(df['condition'] == condition) & \n",
    "                         (df['prompt_type'] == 'baseline')]\n",
    "            baseline_acc = baseline['is_correct'].mean() if len(baseline) > 0 else 0\n",
    "            \n",
    "            # Linear geometries\n",
    "            linear = df[(df['condition'] == condition) & \n",
    "                       (df['prompt_type'].isin(linear_geometries))]\n",
    "            linear_acc = linear['is_correct'].mean() if len(linear) > 0 else 0\n",
    "            \n",
    "            # Non-linear geometries\n",
    "            nonlinear = df[(df['condition'] == condition) & \n",
    "                          (df['prompt_type'].isin(nonlinear_geometries))]\n",
    "            nonlinear_acc = nonlinear['is_correct'].mean() if len(nonlinear) > 0 else 0\n",
    "            \n",
    "            # CoT\n",
    "            cot = df[(df['condition'] == condition) & \n",
    "                    (df['prompt_type'] == 'cot')]\n",
    "            cot_acc = cot['is_correct'].mean() if len(cot) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'condition': condition,\n",
    "                'baseline': baseline_acc,\n",
    "                'linear': linear_acc,\n",
    "                'nonlinear': nonlinear_acc,\n",
    "                'cot': cot_acc,\n",
    "                'linear_improvement': linear_acc - baseline_acc,\n",
    "                'nonlinear_improvement': nonlinear_acc - baseline_acc,\n",
    "                'cot_improvement': cot_acc - baseline_acc\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\n{model_name} - Geometry Comparison:\")\n",
    "        print(\"-\" * 60)\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            print(f\"{row['condition']:12}: Baseline {row['baseline']:.3f} | \"\n",
    "                  f\"Linear {row['linear']:.3f} (Δ={row['linear_improvement']:+.3f}) | \"\n",
    "                  f\"Non-linear {row['nonlinear']:.3f} (Δ={row['nonlinear_improvement']:+.3f}) | \"\n",
    "                  f\"CoT {row['cot']:.3f} (Δ={row['cot_improvement']:+.3f})\")\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    def create_model_comparison_plot(self, all_results: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create visualization comparing all models\"\"\"\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        conditions = ['congruent', 'incongruent', 'random', 'permuted']\n",
    "        \n",
    "        for idx, (model_name, df) in enumerate(all_results.items()):\n",
    "            if idx >= 9:\n",
    "                break\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Calculate accuracy by condition and prompt category\n",
    "            linear = ['numberline', 'road']\n",
    "            nonlinear = ['circle', 'clusters', '3d_space', 'cloud', 'hyperbolic']\n",
    "            \n",
    "            data_for_plot = []\n",
    "            for condition in conditions:\n",
    "                baseline_acc = df[(df['condition'] == condition) & \n",
    "                                 (df['prompt_type'] == 'baseline')]['accuracy'].mean()\n",
    "                linear_acc = df[(df['condition'] == condition) & \n",
    "                              (df['prompt_type'].isin(linear))]['accuracy'].mean()\n",
    "                nonlinear_acc = df[(df['condition'] == condition) & \n",
    "                                 (df['prompt_type'].isin(nonlinear))]['accuracy'].mean()\n",
    "                cot_acc = df[(df['condition'] == condition) & \n",
    "                            (df['prompt_type'] == 'cot')]['accuracy'].mean()\n",
    "                \n",
    "                data_for_plot.append({\n",
    "                    'condition': condition,\n",
    "                    'Baseline': baseline_acc,\n",
    "                    'Linear': linear_acc,\n",
    "                    'Non-linear': nonlinear_acc,\n",
    "                    'CoT': cot_acc\n",
    "                })\n",
    "            \n",
    "            plot_df = pd.DataFrame(data_for_plot)\n",
    "            plot_df.set_index('condition')[['Baseline', 'Linear', 'Non-linear', 'CoT']].plot(\n",
    "                kind='bar', ax=ax, width=0.8\n",
    "            )\n",
    "            \n",
    "            ax.set_title(model_name.replace('_', ' ').title())\n",
    "            ax.set_xlabel('Condition')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Model Comparison: Geometry Effects on Transitive Inference', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def create_prompt_effect_heatmap(self, all_results: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create heatmap showing prompt effects relative to baseline\"\"\"\n",
    "        \n",
    "        # Prompts to compare\n",
    "        prompts_to_compare = ['numberline', 'circle', 'cot']\n",
    "        conditions = ['congruent', 'incongruent', 'random', 'permuted']\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        heatmap_data = {}\n",
    "        \n",
    "        for model_name, df in all_results.items():\n",
    "            model_effects = {}\n",
    "            \n",
    "            for condition in conditions:\n",
    "                # Get baseline accuracy\n",
    "                baseline_acc = df[(df['condition'] == condition) & \n",
    "                                (df['prompt_type'] == 'baseline')]['accuracy'].iloc[0]\n",
    "                \n",
    "                for prompt in prompts_to_compare:\n",
    "                    prompt_acc = df[(df['condition'] == condition) & \n",
    "                                   (df['prompt_type'] == prompt)]['accuracy'].iloc[0]\n",
    "                    \n",
    "                    # Calculate improvement over baseline\n",
    "                    improvement = prompt_acc - baseline_acc\n",
    "                    model_effects[f\"{condition}_{prompt}\"] = improvement * 100  # Convert to percentage points\n",
    "            \n",
    "            heatmap_data[model_name] = model_effects\n",
    "        \n",
    "        # Convert to DataFrame for plotting\n",
    "        heatmap_df = pd.DataFrame(heatmap_data).T\n",
    "        \n",
    "        # Reorder columns for better visualization\n",
    "        column_order = []\n",
    "        for condition in conditions:\n",
    "            for prompt in prompts_to_compare:\n",
    "                column_order.append(f\"{condition}_{prompt}\")\n",
    "        heatmap_df = heatmap_df[column_order]\n",
    "        \n",
    "        # Create figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Subplot 1: Full heatmap\n",
    "        sns.heatmap(heatmap_df, annot=True, fmt='.1f', cmap='RdBu_r', center=0,\n",
    "                    cbar_kws={'label': 'Improvement over baseline (percentage points)'},\n",
    "                    vmin=-10, vmax=10, ax=ax1)\n",
    "        ax1.set_title('Prompt Effects Relative to Baseline (All Conditions)', fontsize=14)\n",
    "        ax1.set_xlabel('Condition and Prompt Type')\n",
    "        ax1.set_ylabel('Model')\n",
    "        \n",
    "        # Format x-axis labels\n",
    "        labels = []\n",
    "        for col in heatmap_df.columns:\n",
    "            condition, prompt = col.rsplit('_', 1)\n",
    "            labels.append(f\"{condition[:4]}_{prompt[:4]}\")\n",
    "        ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        \n",
    "        # Subplot 2: Averaged across conditions\n",
    "        avg_effects = {}\n",
    "        for model_name, df in all_results.items():\n",
    "            model_avg = {}\n",
    "            for prompt in prompts_to_compare:\n",
    "                improvements = []\n",
    "                for condition in conditions:\n",
    "                    baseline_acc = df[(df['condition'] == condition) & \n",
    "                                    (df['prompt_type'] == 'baseline')]['accuracy'].iloc[0]\n",
    "                    prompt_acc = df[(df['condition'] == condition) & \n",
    "                                   (df['prompt_type'] == prompt)]['accuracy'].iloc[0]\n",
    "                    improvements.append((prompt_acc - baseline_acc) * 100)\n",
    "                model_avg[prompt] = np.mean(improvements)\n",
    "            avg_effects[model_name] = model_avg\n",
    "        \n",
    "        avg_df = pd.DataFrame(avg_effects).T\n",
    "        \n",
    "        sns.heatmap(avg_df, annot=True, fmt='.1f', cmap='RdBu_r', center=0,\n",
    "                    cbar_kws={'label': 'Avg improvement (pp)'},\n",
    "                    vmin=-5, vmax=5, ax=ax2)\n",
    "        ax2.set_title('Average Prompt Effects (Across All Conditions)', fontsize=14)\n",
    "        ax2.set_xlabel('Prompt Type')\n",
    "        ax2.set_ylabel('Model')\n",
    "        ax2.set_xticklabels(['Number Line', 'Circle', 'CoT'], rotation=0)\n",
    "        \n",
    "        plt.suptitle('Prompt Strategy Effects: Improvement over Baseline', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('prompt_effects_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PROMPT EFFECT SUMMARY (percentage point improvement over baseline)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for prompt in prompts_to_compare:\n",
    "            print(f\"\\n{prompt.upper()} PROMPT:\")\n",
    "            for condition in conditions:\n",
    "                improvements = []\n",
    "                for model in heatmap_data.keys():\n",
    "                    improvements.append(heatmap_data[model][f\"{condition}_{prompt}\"])\n",
    "                mean_imp = np.mean(improvements)\n",
    "                std_imp = np.std(improvements)\n",
    "                print(f\"  {condition:12}: {mean_imp:+.2f} ± {std_imp:.2f} pp\")\n",
    "        \n",
    "        # Identify best prompt for each condition\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BEST PROMPT BY CONDITION (averaged across models)\")\n",
    "        print(\"=\"*60)\n",
    "        for condition in conditions:\n",
    "            best_prompt = None\n",
    "            best_improvement = -float('inf')\n",
    "            for prompt in prompts_to_compare:\n",
    "                avg_imp = np.mean([heatmap_data[model][f\"{condition}_{prompt}\"] \n",
    "                                  for model in heatmap_data.keys()])\n",
    "                if avg_imp > best_improvement:\n",
    "                    best_improvement = avg_imp\n",
    "                    best_prompt = prompt\n",
    "            print(f\"{condition:12}: {best_prompt} ({best_improvement:+.2f} pp)\")\n",
    "        \n",
    "        return heatmap_df, avg_df\n",
    "\n",
    "    def create_prompt_effect_plots(self, all_results: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create cleaner visualizations of prompt effects\"\"\"\n",
    "        \n",
    "        # Prompts to compare\n",
    "        prompts_to_compare = ['numberline', 'circle', 'cot']\n",
    "        conditions = ['congruent', 'incongruent', 'random', 'permuted']\n",
    "        \n",
    "        # Collect data\n",
    "        all_effects = []\n",
    "        for model_name, df in all_results.items():\n",
    "            for condition in conditions:\n",
    "                baseline_acc = df[(df['condition'] == condition) & \n",
    "                                (df['prompt_type'] == 'baseline')]['accuracy'].iloc[0]\n",
    "                \n",
    "                for prompt in prompts_to_compare:\n",
    "                    prompt_acc = df[(df['condition'] == condition) & \n",
    "                                   (df['prompt_type'] == prompt)]['accuracy'].iloc[0]\n",
    "                    \n",
    "                    all_effects.append({\n",
    "                        'model': model_name,\n",
    "                        'condition': condition,\n",
    "                        'prompt': prompt,\n",
    "                        'improvement': (prompt_acc - baseline_acc) * 100\n",
    "                    })\n",
    "        \n",
    "        effects_df = pd.DataFrame(all_effects)\n",
    "        \n",
    "        # Create figure with 3 subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Plot 1: By prompt type (averaged across conditions)\n",
    "        # Order models: Gemini models first, then Gemma models by size\n",
    "        model_order = [\n",
    "            'gemini_2.5_flash', 'gemini_2.5_flash_lite', 'gemini_2.5_pro',\n",
    "            'gemma3_1b', 'gemma3_4b', 'gemma3_12b', 'gemma3_27b'\n",
    "        ]\n",
    "        # Filter to only models that exist in the data\n",
    "        model_order = [m for m in model_order if m in effects_df['model'].unique()]\n",
    "        \n",
    "        ax1 = axes[0]\n",
    "        prompt_avg = effects_df.groupby(['model', 'prompt'])['improvement'].mean().reset_index()\n",
    "        prompt_pivot = prompt_avg.pivot(index='model', columns='prompt', values='improvement')\n",
    "        # Reorder rows to match model_order\n",
    "        prompt_pivot = prompt_pivot.reindex(model_order)\n",
    "        \n",
    "        x = np.arange(len(prompt_pivot.index))\n",
    "        width = 0.25\n",
    "        \n",
    "        colors = ['#2E7D32', '#D32F2F', '#1976D2']  # Green for numberline, red for circle, blue for cot\n",
    "        for i, prompt in enumerate(['numberline', 'circle', 'cot']):\n",
    "            offset = (i - 1) * width\n",
    "            ax1.bar(x + offset, prompt_pivot[prompt], width, \n",
    "                   label=prompt.capitalize(), color=colors[i], alpha=0.8)\n",
    "        \n",
    "        ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax1.set_xlabel('Model')\n",
    "        ax1.set_ylabel('Improvement over baseline (pp)')\n",
    "        ax1.set_title('Average Prompt Effects Across All Conditions')\n",
    "        ax1.set_xticks(x)\n",
    "        # Format labels nicely\n",
    "        labels = []\n",
    "        for m in prompt_pivot.index:\n",
    "            if 'gemini' in m:\n",
    "                label = m.replace('gemini_2.5_', 'Gemini-').replace('_', ' ')\n",
    "            else:\n",
    "                label = m.replace('gemma3_', 'Gemma-')\n",
    "            labels.append(label)\n",
    "        ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 2: By condition (showing all three prompts)\n",
    "        ax2 = axes[1]\n",
    "        condition_avg = effects_df.groupby(['condition', 'prompt'])['improvement'].mean().reset_index()\n",
    "        condition_pivot = condition_avg.pivot(index='condition', columns='prompt', values='improvement')\n",
    "        \n",
    "        x = np.arange(len(condition_pivot.index))\n",
    "        for i, prompt in enumerate(['numberline', 'circle', 'cot']):\n",
    "            offset = (i - 1) * width\n",
    "            ax2.bar(x + offset, condition_pivot[prompt], width, \n",
    "                   label=prompt.capitalize(), color=colors[i], alpha=0.8)\n",
    "        \n",
    "        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax2.set_xlabel('Condition')\n",
    "        ax2.set_ylabel('Improvement over baseline (pp)')\n",
    "        ax2.set_title('Prompt Effects by Item Type (Averaged Across Models)')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(condition_pivot.index, rotation=0)\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Model size effect (if Gemma models)\n",
    "        ax3 = axes[2]\n",
    "        gemma_models = ['gemma3_1b', 'gemma3_4b', 'gemma3_12b', 'gemma3_27b']\n",
    "        gemma_data = effects_df[effects_df['model'].isin(gemma_models)]\n",
    "        \n",
    "        if len(gemma_data) > 0:\n",
    "            gemma_avg = gemma_data.groupby(['model', 'prompt'])['improvement'].mean().reset_index()\n",
    "            \n",
    "            # Map model names to sizes\n",
    "            size_map = {'gemma3_1b': 1, 'gemma3_4b': 4, 'gemma3_12b': 12, 'gemma3_27b': 27}\n",
    "            gemma_avg['size'] = gemma_avg['model'].map(size_map)\n",
    "            \n",
    "            # Sort by size to ensure lines go from left to right\n",
    "            gemma_avg = gemma_avg.sort_values('size')\n",
    "            \n",
    "            for prompt, color in zip(['numberline', 'circle', 'cot'], colors):\n",
    "                prompt_data = gemma_avg[gemma_avg['prompt'] == prompt].sort_values('size')\n",
    "                ax3.plot(prompt_data['size'], prompt_data['improvement'], \n",
    "                        'o-', label=prompt.capitalize(), color=color, \n",
    "                        linewidth=2, markersize=8, alpha=0.8)\n",
    "            \n",
    "            ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            ax3.set_xlabel('Model Size (B parameters)')\n",
    "            ax3.set_ylabel('Improvement over baseline (pp)')\n",
    "            ax3.set_title('Scaling Effects on Prompt Strategies (Gemma Models)')\n",
    "            ax3.set_xscale('log')\n",
    "            ax3.set_xticks([1, 4, 12, 27])\n",
    "            ax3.set_xticklabels(['1B', '4B', '12B', '27B'])\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'No Gemma models found', \n",
    "                    ha='center', va='center', transform=ax3.transAxes)\n",
    "        \n",
    "        plt.suptitle('Prompt Strategy Effects: Number Line vs Circle vs Chain-of-Thought', \n",
    "                    fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('prompt_effects_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print key insights\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KEY INSIGHTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Which prompt works best overall?\n",
    "        overall_avg = effects_df.groupby('prompt')['improvement'].agg(['mean', 'std'])\n",
    "        print(\"\\nOverall effectiveness (averaged across all models and conditions):\")\n",
    "        for prompt in ['numberline', 'circle', 'cot']:\n",
    "            mean = overall_avg.loc[prompt, 'mean']\n",
    "            std = overall_avg.loc[prompt, 'std']\n",
    "            print(f\"  {prompt:12}: {mean:+.2f} ± {std:.2f} pp\")\n",
    "        \n",
    "        # Best prompt per condition\n",
    "        print(\"\\nBest prompt by condition:\")\n",
    "        for condition in conditions:\n",
    "            cond_data = effects_df[effects_df['condition'] == condition]\n",
    "            best = cond_data.groupby('prompt')['improvement'].mean().idxmax()\n",
    "            best_val = cond_data.groupby('prompt')['improvement'].mean().max()\n",
    "            print(f\"  {condition:12}: {best} ({best_val:+.2f} pp)\")\n",
    "        \n",
    "        return effects_df\n",
    "\n",
    "    def run_analysis(self, model_files: Dict[str, str]):\n",
    "        \"\"\"Run complete analysis for all models\"\"\"\n",
    "        all_results = {}\n",
    "        all_comparisons = {}\n",
    "        \n",
    "        for model_name, results_file in model_files.items():\n",
    "            if not Path(results_file).exists():\n",
    "                print(f\"Skipping {model_name} - file not found: {results_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse results\n",
    "            df = self.parse_model_results(results_file, model_name)\n",
    "            \n",
    "            # Analyze\n",
    "            accuracy_df = self.analyze_single_model(df, model_name)\n",
    "            comparison_df = self.compare_geometries(df, model_name)\n",
    "            \n",
    "            # Store results\n",
    "            all_results[model_name] = accuracy_df\n",
    "            all_comparisons[model_name] = comparison_df\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(f'{model_name}_detailed_results.csv', index=False)\n",
    "            accuracy_df.to_csv(f'{model_name}_accuracy_summary.csv', index=False)\n",
    "            comparison_df.to_csv(f'{model_name}_geometry_comparison.csv', index=False)\n",
    "        \n",
    "        # Create comparison plot\n",
    "        self.create_model_comparison_plot(all_results)\n",
    "        self.create_prompt_effect_heatmap(all_results)\n",
    "        self.create_prompt_effect_plots(all_results)\n",
    "        return all_results, all_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main analysis function\"\"\"\n",
    "    analyzer = LLMResultsAnalyzer()\n",
    "    \n",
    "    # Define model result files\n",
    "    model_files = {\n",
    "        'gemini_2.5_flash': 'llm_output/gemini-2.5-flash-all_prompts.txt',\n",
    "        'gemini_2.5_flash_lite': 'llm_output/gemini-2.5-flash-lite-all_prompts.txt',\n",
    "        'gemini_2.5_pro': 'llm_output/gemini-2.5-pro-all_prompts.txt',\n",
    "        'gemma3_1b': 'llm_output/gemma3_1b-all_prompts.txt',\n",
    "        'gemma3_4b': 'llm_output/gemma3_4b-all_prompts.txt',\n",
    "        'gemma3_12b': 'llm_output/gemma3_12b-all_prompts.txt',\n",
    "        'gemma3_27b': 'llm_output/gemma3_27b-all_prompts.txt'\n",
    "    }\n",
    "    \n",
    "    # Run analysis\n",
    "    all_results, all_comparisons = analyzer.run_analysis(model_files)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nFiles generated:\")\n",
    "    print(\"- [model_name]_detailed_results.csv - Full results for each model\")\n",
    "    print(\"- [model_name]_accuracy_summary.csv - Accuracy by condition and prompt\")\n",
    "    print(\"- [model_name]_geometry_comparison.csv - Linear vs non-linear comparison\")\n",
    "    print(\"- all_models_comparison.png - Visual comparison across all models\")\n",
    "    \n",
    "    return all_results, all_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, comparisons = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Define model result files\n",
    "model_files = {\n",
    "    'gemini_2.5_flash': 'llm_output/gemini-2.5-flash-all_prompts.txt',\n",
    "    'gemini_2.5_flash_lite': 'llm_output/gemini-2.5-flash-lite-all_prompts.txt',\n",
    "    'gemini_2.5_pro': 'llm_output/gemini-2.5-pro-all_prompts.txt',\n",
    "    'gemma3_1b': 'llm_output/gemma3_1b-all_prompts.txt',\n",
    "    'gemma3_4b': 'llm_output/gemma3_4b-all_prompts.txt',\n",
    "    'gemma3_12b': 'llm_output/gemma3_12b-all_prompts.txt',\n",
    "    'gemma3_27b': 'llm_output/gemma3_27b-all_prompts.txt'\n",
    "}\n",
    "\n",
    "analyzer = LLMResultsAnalyzer()\n",
    "# Run analysis\n",
    "all_results, all_comparisons = analyzer.run_analysis(model_files)\n",
    "\n",
    "analyzer.create_model_comparison_plot(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    'gemini_2.5_flash': 'llm_output/gemini-2.5-flash-all_prompts.txt',\n",
    "    'gemini_2.5_flash_lite': 'llm_output/gemini-2.5-flash-lite-all_prompts.txt',\n",
    "    'gemini_2.5_pro': 'llm_output/gemini-2.5-pro-all_prompts.txt',\n",
    "    'gemma3_1b': 'llm_output/gemma3_1b-all_prompts.txt',\n",
    "    'gemma3_4b': 'llm_output/gemma3_4b-all_prompts.txt',\n",
    "    'gemma3_12b': 'llm_output/gemma3_12b-all_prompts.txt',\n",
    "    'gemma3_27b': 'llm_output/gemma3_27b-all_prompts.txt'\n",
    "}\n",
    "\n",
    "model_names = list(model_files.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    df = pd.read_csv(model_names[i] + '_accuracy_summary.csv')\n",
    "    \n",
    "    print(model_names[i])\n",
    "    \n",
    "    # Preprocess\n",
    "    df['yerr_lower'] = df['accuracy'] - df['ci_low']\n",
    "    df['yerr_upper'] = df['ci_high'] - df['accuracy']\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    g = sns.barplot(ax=ax, data=df, x='prompt_type', y='accuracy', hue='condition')\n",
    "    # g._legend.set_bbox_to_anchor((1.1, 0.5))  # x, y — move to right center\n",
    "    \n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.title(model_name)\n",
    "    # Add error bars manually\n",
    "    # Iterate over bars and add errorbars\n",
    "    for bar, (_, row) in zip(ax.patches, df.iterrows()):\n",
    "        # Calculate the center of each bar\n",
    "        x = bar.get_x() + bar.get_width() / 2\n",
    "        y = bar.get_height()\n",
    "        \n",
    "        # Add error bar using asymmetric values\n",
    "        ax.errorbar(\n",
    "            x, y,\n",
    "            yerr=[[row['yerr_lower']], [row['yerr_upper']]],\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=5,\n",
    "            linewidth=1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    fig.legend(bbox_to_anchor=(1.15,0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'accuracy_{model_name}.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model_name in enumerate(model_names):\n",
    "    \n",
    "    df = pd.read_csv(model_name + '_geometry_comparison.csv')\n",
    "    \n",
    "    df\n",
    "    df_pivoted = df[['condition', 'baseline', 'linear', 'nonlinear', 'cot']].melt(id_vars='condition',\n",
    "                                                                                 var_name='prompt_type',\n",
    "                                                                                 value_name='accuracy')\n",
    "    df_pivoted\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ylim((0,1))\n",
    "    sns.barplot(ax=ax, data=df_pivoted, x='prompt_type', y='accuracy', hue='condition', palette='husl')\n",
    "    plt.title(model_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'geometry_comparison_{model_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['condition', 'baseline', 'linear', 'nonlinear', 'cot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "n_unclear_responses = {}\n",
    "for model_name in model_names:\n",
    "    df = pd.read_csv(model_name + '_detailed_results.csv').reset_index()\n",
    "\n",
    "    n_unclear_responses[model_name] = np.sum(df['predicted']=='unclear')\n",
    "    dfs.append(df)\n",
    "n_unclear_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detailed_results_df = pd.concat(dfs)\n",
    "all_detailed_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=all_detailed_results_df, y='response_length', x='model', ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=all_detailed_results_df, y='response_length', x='model', hue='condition', ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Note that some models (most notably flash lite) respond longer for incongruent and permuted. I suspect that it notices some conflict, while listing the items in order, between real-world info and what's in the list, and discusses this in the answer, or something like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=all_detailed_results_df, y='response_length', x='model', hue='prompt_type', ax=ax, palette='husl')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "next, look at distribution of response lengths, per model. are there two types of responses (quick answer versus reasoning trace), or is it more a continuum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "mn = model_names[0]\n",
    "sns.histplot(data=all_detailed_results_df[all_detailed_results_df.model==mn], x='response_length')\n",
    "plt.title(mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "looks very bimodal: some very short responses and then a distribution of lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above plot out for all models \n",
    "for i, model_name in enumerate(model_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.histplot(ax=ax, data=all_detailed_results_df[all_detailed_results_df.model==model_name], x='response_length')\n",
    "    plt.title(model_name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'response_length_{model_name}.png')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Redo analysis, keeping only short responses \n",
    "The reasoning here is that we want to see if the geometry can help as a scaffold *without any CoT thinking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_length_threshold = 20\n",
    "short_response_df = all_detailed_results_df[all_detailed_results_df['response_length'] < response_length_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_counts = short_response_df.groupby(['model'])['index'].count()\n",
    "\n",
    "models_to_keep = model_counts[model_counts >= 1000].index\n",
    "\n",
    "short_response_df = short_response_df[short_response_df['model'].isin(models_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for model_name, results_file in model_files.items():\n",
    "    if not Path(results_file).exists():\n",
    "        print(f\"Skipping {model_name} - file not found: {results_file}\")\n",
    "        continue\n",
    "    \n",
    "    df = short_response_df[short_response_df.model==model_name]\n",
    "    if len(df) < 1000:\n",
    "        print(f'too few datapoints for {model_name}. skipping')\n",
    "        continue\n",
    "    \n",
    "    # Analyze\n",
    "    accuracy_df = analyzer.analyze_single_model(df, model_name)\n",
    "    comparison_df = analyzer.compare_geometries(df, model_name)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[model_name] = accuracy_df\n",
    "    all_comparisons[model_name] = comparison_df\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(f'{model_name}_detailed_results_only_short_responses.csv', index=False)\n",
    "    accuracy_df.to_csv(f'{model_name}_accuracy_summary_only_short_responses.csv', index=False)\n",
    "    comparison_df.to_csv(f'{model_name}_geometry_comparison_only_short_responses.csv', index=False)\n",
    "\n",
    "\n",
    "analyzer.create_model_comparison_plot(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    df = pd.read_csv(model_names[i] + '_accuracy_summary_only_short_responses.csv')\n",
    "    \n",
    "    print(model_names[i])\n",
    "    \n",
    "    # Preprocess\n",
    "    df['yerr_lower'] = df['accuracy'] - df['ci_low']\n",
    "    df['yerr_upper'] = df['ci_high'] - df['accuracy']\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    g = sns.barplot(ax=ax, data=df, x='prompt_type', y='accuracy', hue='condition')\n",
    "    # g._legend.set_bbox_to_anchor((1.1, 0.5))  # x, y — move to right center\n",
    "    \n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.title(model_name)\n",
    "    # Add error bars manually\n",
    "    # Iterate over bars and add errorbars\n",
    "    for bar, (_, row) in zip(ax.patches, df.iterrows()):\n",
    "        # Calculate the center of each bar\n",
    "        x = bar.get_x() + bar.get_width() / 2\n",
    "        y = bar.get_height()\n",
    "        \n",
    "        # Add error bar using asymmetric values\n",
    "        ax.errorbar(\n",
    "            x, y,\n",
    "            yerr=[[row['yerr_lower']], [row['yerr_upper']]],\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=5,\n",
    "            linewidth=1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    fig.legend(bbox_to_anchor=(1.15,0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'accuracy_{model_name}_only_short_responses.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## conclusion: too few of the responses are short to make a conclusion.\n",
    "For the big models (already very high performance), selecting the short responses pushes them even more towards ceiling performance. to test this properly, we'll need to force the model to not use CoT.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Difference plots \n",
    "To make the effects a bit more visible, try some plots of differences between condition, rather than raw accuracy bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.create_prompt_effect_heatmap(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
